# LITELLM PROXY DEPENDENCIES #
anyio==4.6.0 # openai + http req.
openai==1.51.2 # openai req. 
fastapi==0.115.0 # server dep
backoff==2.2.1 # server dep
pyyaml==6.0.2 # server dep
uvicorn==0.31.1 # server dep
gunicorn==23.0.0 # server dep
boto3==1.7.12 # aws bedrock/sagemaker calls
redis==5.1.1 # caching
numpy==2.1.2 # semantic caching
prisma==0.15.0 # for db
mangum==0.19.0 # for aws lambda functions
pynacl==1.5.0 # for encrypting keys
google-cloud-aiplatform==1.70.0 # for vertex ai calls
anthropic[vertex]==0.36.0
google-generativeai==0.8.3 # for vertex ai calls
async_generator==1.10.0 # for async ollama calls
langfuse==2.52.0 # for langfuse self-hosted logging
prometheus_client==0.21.0 # for /metrics endpoint on proxy
orjson==3.10.7 # fast /embedding responses
apscheduler==3.10.4 # for resetting budget in background 
fastapi-sso==0.15.0 # admin UI, SSO
pyjwt[crypto]==2.9.0
python-multipart==0.0.12 # admin UI
Pillow==10.4.0
azure-ai-contentsafety==1.0.0 # for azure content safety
azure-identity==1.19.0 # for azure content safety
opentelemetry-api==1.27.0
opentelemetry-sdk==1.27.0
opentelemetry-exporter-otlp==1.27.0
sentry_sdk==2.16.0 # for sentry error handling
detect-secrets==1.5.0 # Enterprise - secret detection / masking in LLM requests
cryptography==43.0.1

### LITELLM PACKAGE DEPENDENCIES
python-dotenv==1.0.1 # for env 
tiktoken==0.8.0 # for calculating usage
importlib-metadata==8.4.0 # for random utils
tokenizers==0.20.1 # for calculating usage
click==8.1.7 # for proxy cli 
jinja2==3.1.4 # for prompt templates
certifi==2024.8.30 # [TODO] clean up 
aiohttp==3.10.9 # for network calls
aioboto3==12.3.0 # for async sagemaker calls
tenacity==9.0.0  # for retrying requests, when litellm.num_retries set
pydantic==2.9.2 # proxy + openai req.
jsonschema==4.23.0 # validating json schema
websockets==13.1 # for realtime API
####